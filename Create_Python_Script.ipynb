{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Create the Python Script\n",
    "\n",
    "In the cell below, you will need to complete the Python script and run the cell to generate the file using the magic `%%writefile` command. Your main task is to complete the following methods for the `PersonDetect` class:\n",
    "* `load_model`\n",
    "* `predict`\n",
    "* `draw_outputs`\n",
    "* `preprocess_outputs`\n",
    "* `preprocess_inputs`\n",
    "\n",
    "For your reference, here are all the arguments used for the argument parser in the command line:\n",
    "* `--model`:  The file path of the pre-trained IR model, which has been pre-processed using the model optimizer. There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware.\n",
    "* `--device`: The type of hardware you want to load the model on (CPU, GPU, MYRIAD, HETERO:FPGA,CPU)\n",
    "* `--video`: The file path of the input video.\n",
    "* `--output_path`: The location where the output stats and video file with inference needs to be stored (results/[device]).\n",
    "* `--max_people`: The max number of people in queue before directing a person to another queue.\n",
    "* `--threshold`: The probability threshold value for the person detection. Optional arg; default value is 0.60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"People Counter.\"\"\"\n",
    "\"\"\"\n",
    " Copyright (c) 2018 Intel Corporation.\n",
    " Permission is hereby granted, free of charge, to any person obtaining\n",
    " a copy of this software and associated documentation files (the\n",
    " \"Software\"), to deal in the Software without restriction, including\n",
    " without limitation the rights to use, copy, modify, merge, publish,\n",
    " distribute, sublicense, and/or sell copies of the Software, and to\n",
    " permit person to whom the Software is furnished to do so, subject to\n",
    " the following conditions:\n",
    " The above copyright notice and this permission notice shall be\n",
    " included in all copies or substantial portions of the Software.\n",
    " THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    " EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    " MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
    " NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n",
    " LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n",
    " OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
    " WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import socket\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import logging as log\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from inference import Network\n",
    "\n",
    "# MQTT server environment variables\n",
    "HOSTNAME = socket.gethostname()\n",
    "IPADDRESS = socket.gethostbyname(HOSTNAME)\n",
    "MQTT_HOST = IPADDRESS\n",
    "MQTT_PORT = 3001\n",
    "MQTT_KEEPALIVE_INTERVAL = 60\n",
    "\n",
    "def build_argparser():\n",
    "    \"\"\"\n",
    "    Parse command line arguments.\n",
    "\n",
    "    :return: command line arguments\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-m\", \"--model\", required=True, type=str,\n",
    "                        help=\"Path to an xml file with a trained model.\")\n",
    "    parser.add_argument(\"-i\", \"--input\", required=True, type=str,\n",
    "                        help=\"Path to image or video file\")\n",
    "    parser.add_argument(\"-l\", \"--cpu_extension\", required=False, type=str,\n",
    "                        default=None,\n",
    "                        help=\"MKLDNN (CPU)-targeted custom layers.\"\n",
    "                             \"Absolute path to a shared library with the\"\n",
    "                             \"kernels impl.\")\n",
    "    parser.add_argument(\"-d\", \"--device\", type=str, default=\"CPU\",\n",
    "                        help=\"Specify the target device to infer on: \"\n",
    "                             \"CPU, GPU, FPGA or MYRIAD is acceptable. Sample \"\n",
    "                             \"will look for a suitable plugin for device \"\n",
    "                             \"specified (CPU by default)\")\n",
    "    parser.add_argument(\"-pt\", \"--prob_threshold\", type=float, default=0.55,\n",
    "                        help=\"Probability threshold for detections filtering\"\n",
    "                        \"(0.55 by default)\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "def connect_mqtt():\n",
    "    # Connect to the MQTT server\n",
    "    client = mqtt.Client()\n",
    "    client.connect(MQTT_HOST, MQTT_PORT, MQTT_KEEPALIVE_INTERVAL)\n",
    "    return client\n",
    "\n",
    "def draw_outputs(coords, frame, initial_w, initial_h, x, k):\n",
    "        # Draw output\n",
    "        # print('Draw Output...')\n",
    "        current_count = 0     \n",
    "        ed = x\n",
    "        for obj in coords[0][0]:\n",
    "            # Draw bounding box for object when it's probability is more than the specified threshold\n",
    "            if obj[2] > prob_threshold:\n",
    "                xmin = int(obj[3] * initial_w)\n",
    "                ymin = int(obj[4] * initial_h)\n",
    "                xmax = int(obj[5] * initial_w)\n",
    "                ymax = int(obj[6] * initial_h)\n",
    "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 1)\n",
    "                current_count = current_count + 1\n",
    "                #print(current_count)\n",
    "                \n",
    "                c_x = frame.shape[1]/2\n",
    "                c_y = frame.shape[0]/2    \n",
    "                mid_x = (xmax + xmin)/2\n",
    "                mid_y = (ymax + ymin)/2\n",
    "                \n",
    "                # Calculating distance \n",
    "                ed =  math.sqrt(math.pow(mid_x - c_x, 2) +  math.pow(mid_y - c_y, 2) * 1.0) \n",
    "                k = 0\n",
    "\n",
    "        if current_count < 1:\n",
    "            k += 1\n",
    "            \n",
    "        if ed>0 and k < 10:\n",
    "            current_count = 1 \n",
    "            k += 1 \n",
    "            if k > 100:\n",
    "                k = 0\n",
    "                \n",
    "        return frame, current_count, ed, k\n",
    "\n",
    "def infer_on_stream(args, client):\n",
    "    # Initialise the class\n",
    "    infer_network = Network()\n",
    "    # Set Probability threshold for detections\n",
    "    model=args.model\n",
    "    video_file=args.input    \n",
    "    extn=args.cpu_extension\n",
    "    device=args.device\n",
    "    #prob_threshold = args.prob_threshold\n",
    "    \n",
    "    # Flag for the input image\n",
    "    single_img_flag = False\n",
    "\n",
    "    start_time = 0\n",
    "    cur_request_id = 0\n",
    "    last_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    # Load the model through `infer_network` \n",
    "    n, c, h, w = infer_network.load_model(model, device, 1, 1, cur_request_id, extn)[1]\n",
    "\n",
    "    # Handle the input stream\n",
    "    if video_file == 'CAM': # Check for live feed\n",
    "        input_stream = 0\n",
    "\n",
    "    elif video_file.endswith('.jpg') or video_file.endswith('.bmp') :    # Check for input image\n",
    "        single_img_flag = True\n",
    "        input_stream = video_file\n",
    "\n",
    "    else:     # Check for video file\n",
    "        input_stream = video_file\n",
    "        assert os.path.isfile(video_file), \"Specified input file doesn't exist\"\n",
    "    \n",
    "    try:\n",
    "        cap=cv2.VideoCapture(video_file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Cannot locate video file: \"+ video_file)\n",
    "    except Exception as e:\n",
    "        print(\"Something else went wrong with the video file: \", e)\n",
    "        \n",
    "    global initial_w, initial_h, prob_threshold\n",
    "    total_count = 0  \n",
    "    duration = 0\n",
    "    \n",
    "    initial_w = cap.get(3)\n",
    "    initial_h = cap.get(4)\n",
    "    prob_threshold = args.prob_threshold\n",
    "    temp = 0\n",
    "    tk = 0\n",
    "    \n",
    "    # Loop until stream is over\n",
    "    while cap.isOpened():\n",
    "        # Read from the video capture\n",
    "        flag, frame = cap.read()\n",
    "        if not flag:\n",
    "            break\n",
    "        key_pressed = cv2.waitKey(60)\n",
    "        # Pre-process the image as needed\n",
    "        # Start async inference\n",
    "        image = cv2.resize(frame, (w, h))\n",
    "        # Change data layout from HWC to CHW\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = image.reshape((n, c, h, w))\n",
    "        \n",
    "        # Start asynchronous inference for specified request\n",
    "        inf_start = time.time()\n",
    "        infer_network.exec_net(cur_request_id, image)\n",
    "        \n",
    "        color = (255,0,0)\n",
    "\n",
    "        # Wait for the result\n",
    "        if infer_network.wait(cur_request_id) == 0:\n",
    "            det_time = time.time() - inf_start\n",
    "\n",
    "            # Get the results of the inference request \n",
    "            result = infer_network.get_output(cur_request_id)\n",
    "            \n",
    "            # Draw Bounting Box\n",
    "            frame, current_count, d, tk = draw_outputs(result, frame, initial_w, initial_h, temp, tk)\n",
    "            \n",
    "            # Printing Inference Time \n",
    "            inf_time_message = \"Inference time: {:.3f}ms\".format(det_time * 1000)\n",
    "            cv2.putText(frame, inf_time_message, (15, 15), cv2.FONT_HERSHEY_COMPLEX, 0.5, color, 1)\n",
    "            \n",
    "            # Calculate and send relevant information \n",
    "            if current_count > last_count: # New entry\n",
    "                start_time = time.time()\n",
    "                total_count = total_count + current_count - last_count\n",
    "                client.publish(\"person\", json.dumps({\"total\": total_count}))            \n",
    "            \n",
    "            if current_count < last_count: # Average Time\n",
    "                duration = int(time.time() - start_time) \n",
    "                client.publish(\"person/duration\", json.dumps({\"duration\": duration}))\n",
    "           \n",
    "            # Adding overlays to the frame            \n",
    "            txt2 = \"Distance: %d\" %d + \" Lost frame: %d\" %tk\n",
    "            cv2.putText(frame, txt2, (15, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, color, 1)\n",
    "            \n",
    "            txt2 = \"Current count: %d \" %current_count\n",
    "            cv2.putText(frame, txt2, (15, 45), cv2.FONT_HERSHEY_COMPLEX, 0.5, color, 1)\n",
    "\n",
    "            if current_count > 3:\n",
    "                txt2 = \"Alert! Maximum count reached\"\n",
    "                (text_width, text_height) = cv2.getTextSize(txt2, cv2.FONT_HERSHEY_COMPLEX, 0.5, thickness=1)[0]\n",
    "                text_offset_x = 10\n",
    "                text_offset_y = frame.shape[0] - 10\n",
    "                # make the coords of the box with a small padding of two pixels\n",
    "                box_coords = ((text_offset_x, text_offset_y + 2), (text_offset_x + text_width, text_offset_y - text_height - 2))\n",
    "                cv2.rectangle(frame, box_coords[0], box_coords[1], (0, 0, 0), cv2.FILLED)\n",
    "                \n",
    "                cv2.putText(frame, txt2, (text_offset_x, text_offset_y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)\n",
    "            \n",
    "            client.publish(\"person\", json.dumps({\"count\": current_count})) # People Count\n",
    "\n",
    "            last_count = current_count\n",
    "            temp = d\n",
    "\n",
    "            if key_pressed == 27:\n",
    "                break\n",
    "\n",
    "        # Send the frame to the FFMPEG server\n",
    "        sys.stdout.buffer.write(frame)  \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        #Save the Image\n",
    "        if single_img_flag:\n",
    "            cv2.imwrite('output_image.jpg', frame)\n",
    "       \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    client.disconnect()\n",
    "    infer_network.clean()\n",
    "\n",
    "def main():\n",
    "    # Grab command line args\n",
    "    args = build_argparser().parse_args()\n",
    "    # Connect to the MQTT server\n",
    "    client = connect_mqtt()\n",
    "    # Perform inference on the input stream\n",
    "    infer_on_stream(args, client)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "\n",
    "Now that you've run the above cell and created your Python script, you will create your job submission shell script in the next workspace.\n",
    "\n",
    "**Note**: As a reminder, if you need to make any changes to the Python script, you can come back to this workspace to edit and run the above cell to overwrite the file with your changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
